{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here put stdio.h code , because header file is preprossesd before lexical analysis <br>\n",
    "So this line = \"#include <stdio.h>\" is not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def count_tokens(c_code):\n",
    "    # Define regular expressions for different types of tokens\n",
    "    patterns = {\n",
    "        'keywords': r'\\b(int|return)\\b',\n",
    "        'identifiers': r'\\b[a-zA-Z_]\\w*\\b',\n",
    "        'operators': r'[+\\-*/=,;(){}]',\n",
    "        'literals': r'\\b\\d+\\b',\n",
    "        'strings': r'\"[^\"]*\"'\n",
    "    }\n",
    "\n",
    "    # Combine all patterns into a single pattern\n",
    "    combined_pattern = '|'.join(f'({pattern})' for pattern in patterns.values())\n",
    "\n",
    "    # Match the pattern in the C code\n",
    "    matches = re.findall(combined_pattern, c_code)\n",
    "\n",
    "    # Flatten the list of matches and join them into a comma-separated string\n",
    "    tokens_as_string = []\n",
    "    for match in matches:\n",
    "        unique_tokens = set()\n",
    "        for token in match:\n",
    "            if token:\n",
    "                unique_tokens.add(token)\n",
    "        tokens_as_string.append(', '.join(sorted(unique_tokens)))\n",
    "\n",
    "    token_count = len(tokens_as_string)\n",
    "\n",
    "    return token_count, tokens_as_string\n",
    "\n",
    "# # Read the C file\n",
    "# def read_c_file(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, 'r') as file:\n",
    "#             c_code = file.read()\n",
    "#         return c_code\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Error: File '{file_path}' not found.\")\n",
    "#         return None\n",
    "\n",
    "# # Example C file path\n",
    "# c_file_path = 'example.c'\n",
    "# # Read the content of the C file\n",
    "# c_code_content = read_c_file(c_file_path)\n",
    "# # If content is successfully read, count tokens and print the result\n",
    "# if c_code_content is not None:\n",
    "#     total_tokens, token_list = count_tokens(c_code_content)\n",
    "#     print(f'Total number of tokens: {total_tokens}')\n",
    "#     print('Tokens:', token_list)\n",
    "\n",
    "\n",
    "\n",
    "# Example C code\n",
    "c_code = \"\"\"\n",
    "int main()\n",
    "{\n",
    "  int a = 10, b = 20;\n",
    "  printf(\"asdsajd sad dj\", a + b);\n",
    "  return 0;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Count tokens and print the result\n",
    "total_tokens, token_list = count_tokens(c_code)\n",
    "print(f'Total number of tokens: {total_tokens}')\n",
    "print('Tokens = ', token_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
